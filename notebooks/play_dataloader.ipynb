{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577c2946-fd62-47c5-ae54-b45715711cde",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b957fe-2038-4b0d-bc0b-cc09b1d03d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from loquacious_set_prepare import load_datasets\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.dataio.sampler import DynamicBatchSampler\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# !pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac63de-29b0-4170-b8f6-a0cd7f6d5333",
   "metadata": {},
   "source": [
    "# Get dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cbf3a-e9cf-40ba-8a44-e1047cf8903a",
   "metadata": {},
   "source": [
    "## load hparams and define data_prep function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63681c3-fe45-4925-a527-4aac8b1d0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since speechbrain/LoquaciousSet couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'small' at /local_disk/apollon/rwhetten/hf_root/datasets/speechbrain___loquacious_set/small/0.0.0/720eb654f18f115053c4aea052133234a8bb34b7 (last modified on Mon Jul 14 14:10:50 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aea47202eb4a01be78977de2493d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hparams_file = \"/local_disk/apollon/rwhetten/sss_data_selection/hparams/data_select.yaml\"\n",
    "overrides = {\n",
    "    \"tls_subset\": \"small\",\n",
    "    \"hf_hub\": \"speechbrain/LoquaciousSet\",\n",
    "    \"hf_caching_dir\": \"/local_disk/apollon/rwhetten/hf_root/datasets\",\n",
    "    \"save_int\": 5,\n",
    "    \"ckpt_path\": \"ckpt.pkl\",\n",
    "    \"feature_function_name\": \"mel\",\n",
    "}\n",
    "\n",
    "with open(hparams_file, encoding=\"utf-8\") as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "# tls_subset=\"small\"\n",
    "# hf_hub=\"speechbrain/LoquaciousSet\"\n",
    "# hf_caching_dir=\"/local_disk/apollon/rwhetten/hf_root/datasets\"\n",
    "\n",
    "hf_data_dict = load_datasets(\n",
    "    hparams[\"tls_subset\"],\n",
    "    hparams[\"hf_hub\"],\n",
    "    hparams[\"hf_caching_dir\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7091f13-c8ad-4c2e-9c07-5413511b5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_dict, hparams):\n",
    "    # We must rename the 'id' column because SpeechBrain sampling use this\n",
    "    # name for the sampler already, also it's not an id, but an audio_path.\n",
    "    train_data = hf_data_dict[\"train\"].rename_column(\"ID\", \"audio_id\")\n",
    "    # create list of durations for the dynamic batch sampler, for speed\n",
    "    train_len_list = list(train_data.select_columns(\"duration\")[\"duration\"])\n",
    "    # create dataset obj\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_arrow_dataset(\n",
    "        train_data,\n",
    "    )\n",
    "\n",
    "    datasets = [train_data]\n",
    "\n",
    "    # create and add pipeline to datasets\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        sig = read_audio(wav[\"bytes\"])\n",
    "        return sig\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets,\n",
    "        [\"id\", \"audio_id\", \"sig\"],\n",
    "    )\n",
    "\n",
    "    # for now just working with one dataset\n",
    "    dataset = datasets[0]\n",
    "    dynamic_hparams_train = hparams[\"dynamic_batch_sampler_train\"]\n",
    "\n",
    "    # create dynamic batch sampler\n",
    "    train_batch_sampler = DynamicBatchSampler(\n",
    "        train_data,\n",
    "        length_func=lambda x: x[\"duration\"],\n",
    "        lengths_list=train_len_list,\n",
    "        **dynamic_hparams_train,\n",
    "    )\n",
    "\n",
    "    train_loader_kwargs = {\n",
    "        \"batch_sampler\": train_batch_sampler,\n",
    "        \"num_workers\": hparams[\"num_workers\"],\n",
    "    }\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = sb.dataio.dataloader.make_dataloader(\n",
    "        dataset, **train_loader_kwargs\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa2a4b-414d-4770-9374-142918bc0a3c",
   "metadata": {},
   "source": [
    "## do data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fa6a4c5c-a53a-4a7e-8b02-f934019c216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data_prep(hf_data_dict, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c045fa85-bb58-4d10-b367-fd68a04a4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.utils.checkpoints import Checkpointer\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9222dee9-2f30-4e22-a138-eda3acdd9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sb.utils.checkpoints.register_checkpoint_hooks\n",
    "class Step:\n",
    "    def __init__(self,step=0,end=0):\n",
    "        self.step = step\n",
    "        self.end = end\n",
    "\n",
    "    def __call__(self):\n",
    "        self.step = self.step + 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"step count: {self.step}\"\n",
    "        \n",
    "    @sb.utils.checkpoints.mark_as_saver\n",
    "    def _save(self, path):\n",
    "        save_dict = {\n",
    "            \"step\": self.step,\n",
    "            \"end\": self.end,\n",
    "        }\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as w:\n",
    "            w.write(yaml.dump(save_dict))\n",
    "\n",
    "    @sb.utils.checkpoints.mark_as_loader\n",
    "    def _recover(self, path, end_of_epoch):\n",
    "        del end_of_epoch\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            save_dict = yaml.safe_load(f)\n",
    "        self.step = save_dict[\"step\"]\n",
    "        self.end = save_dict[\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "119038b9-a039-4b71-b415-c8f92b3de261",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/local_disk/apollon/rwhetten/sss_data_selection/save_dl/\"\n",
    "step = Step(0)\n",
    "checkpointer = Checkpointer(save_path, {\"dataloader\": dl, \"step\": step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "50888878-33f3-4af7-a7ee-39004cd533e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeddc04b08b448db960bc97d50e1845d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 70412\n",
      "1, 83\n",
      "2, 11028\n",
      "3, 41\n",
      "saving\n",
      "4, 15375\n",
      "5, 7825\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(dl)):\n",
    "    print(f\"{i}, {batch.id[0]}\")\n",
    "    step()\n",
    "    if i == 5:\n",
    "        break\n",
    "    if i == 3:\n",
    "        print('saving')\n",
    "        _ = checkpointer.save_checkpoint(end_of_epoch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "96ece324-b843-4089-8c25-5e5535792ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndl = data_prep(hf_data_dict, hparams)\n",
    "nstep=Step()\n",
    "ncheckpointer = Checkpointer(save_path, {\"dataloader\": ndl, \"step\": nstep})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0af14622-0c61-459a-8c83-5bf62c247784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "else\n"
     ]
    }
   ],
   "source": [
    "_ = ncheckpointer.recover_if_possible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c2dfffe4-e2d0-4eb1-b317-c33ad3671974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step count: 100\n"
     ]
    }
   ],
   "source": [
    "print(nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "99769334-6f22-414c-bcf3-adcc2684bc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndl._speechbrain_recovery_skip_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c0ed2ce1-201a-4384-b204-72a165d97d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1847857f78cf484fbc6b4708a9e15b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  3%|##8                                                                                         | 100/3174 [0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(\n",
    "    ndl,\n",
    "    initial=nstep.step,\n",
    "    dynamic_ncols=True,\n",
    ") as t:\n",
    "    for batch in t:\n",
    "        # print(nstep)\n",
    "        # print(f\"{nstep.step}, {batch.id[0]}\")\n",
    "        nstep()\n",
    "        if nstep.step == 200:\n",
    "            break\n",
    "        # if i == 25:\n",
    "        #     _ = ncheckpointer.save_checkpoint(end_of_epoch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367d6ef-cee1-41f6-9fd3-1ba6372c1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert next(iter(ndl)) == dataset[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
