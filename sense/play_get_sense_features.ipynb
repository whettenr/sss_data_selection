{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb5ce77-9bc5-46af-ae0d-e257d1e72f8b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd11524-3dbd-4ad5-9968-aca7e99676a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from loquacious_set_prepare import load_datasets\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.dataio.sampler import DynamicBatchSampler\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers.models.seamless_m4t.feature_extraction_seamless_m4t import SeamlessM4TFeatureExtractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb7e04-dc27-42e2-800a-e2ebd8216c65",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bad22e3-f872-4d64-b023-1f12fefbd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/local_disk/apollon/rwhetten/sss_data_selection/sense/data_select.yaml\"\n",
    "overrides = {\n",
    "    \"tls_subset\": \"small\",\n",
    "    \"hf_hub\": \"speechbrain/LoquaciousSet\",\n",
    "    \"hf_caching_dir\": \"/local_disk/apollon/rwhetten/hf_root/datasets\",\n",
    "    \"save_int\": 5,\n",
    "    \"ckpt_path\": \"ckpt.pkl\",\n",
    "    \"feature_function_name\": \"text_emb\",\n",
    "    \"sense_location\": \"/data/coros3/smdhaffar/SENSE/CKPT+checkpoint_epoch10/\",\n",
    "    \"output_folder\": \"/local_disk/apollon/rwhetten/sss_data_selection/sense/pt_store\",\n",
    "    \"max_batch_length_train\": 100,\n",
    "}\n",
    "\n",
    "# brain.ckpt  counter.ckpt           lr_annealing_adam.ckpt     model.ckpt\n",
    "# CKPT.yaml   dataloader-TRAIN.ckpt  lr_annealing_wav2vec.ckpt  wav2vec2.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7f90831-3cd8-4a26-9dc6-670daaaa2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hparams_file, encoding=\"utf-8\") as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c61918c-2a48-4c9f-b667-e68b5fb3288d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfd6e2a96814e94a3316c7b78056d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b1cdd5300b431b972f315b88715729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d3d9139a9940eaa70b45e84c21bc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<speechbrain.dataio.dataset.DynamicItemDataset object at 0x7f2186d23d10>\n"
     ]
    }
   ],
   "source": [
    "# tls_subset=\"small\"\n",
    "# hf_hub=\"speechbrain/LoquaciousSet\"\n",
    "# hf_caching_dir=\"/local_disk/apollon/rwhetten/hf_root/datasets\"\n",
    "\n",
    "hf_data_dict = load_datasets(\n",
    "    hparams[\"tls_subset\"],\n",
    "    hparams[\"hf_hub\"],\n",
    "    hparams[\"hf_caching_dir\"],\n",
    ")\n",
    "\n",
    "# We must rename the 'id' column because SpeechBrain sampling use this\n",
    "# name for the sampler already, also it's not an id, but an audio_path.\n",
    "train_data = hf_data_dict[\"train\"].rename_column(\"ID\", \"audio_id\")\n",
    "# create list of durations for the dynamic batch sampler, for speed\n",
    "train_len_list = list(train_data.select_columns(\"duration\")[\"duration\"])\n",
    "# create dataset obj\n",
    "train_data = sb.dataio.dataset.DynamicItemDataset.from_arrow_dataset(\n",
    "    train_data,\n",
    ")\n",
    "\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0417af9b-1189-4f2b-8221-c89f3b30187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and add pipeline to datasets\n",
    "datasets = [train_data]\n",
    "\n",
    "@sb.utils.data_pipeline.takes(\"wav\")\n",
    "@sb.utils.data_pipeline.provides(\"sig\")\n",
    "def audio_pipeline(wav):\n",
    "    feature_size = 80\n",
    "    sampling_rate = 16000\n",
    "    num_mel_bins = 80\n",
    "    padding_value = 0.0\n",
    "    stride = 2\n",
    "    \n",
    "    feature_extractor = SeamlessM4TFeatureExtractor(\n",
    "        feature_size=feature_size,\n",
    "        sampling_rate=sampling_rate,\n",
    "        num_mel_bins=num_mel_bins,\n",
    "        padding_value=padding_value,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    sig = read_audio(wav[\"bytes\"])\n",
    "    # np_wav = np.array(sig)\n",
    "    features = feature_extractor(sig,sampling_rate=16000)[\"input_features\"][0]\n",
    "    return torch.Tensor(features)\n",
    "\n",
    "sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
    "\n",
    "\n",
    "# 3. Define text pipeline:\n",
    "@sb.utils.data_pipeline.takes(\"text\")\n",
    "@sb.utils.data_pipeline.provides(\"wrd\")\n",
    "def text_pipeline(wrd):\n",
    "    yield wrd.lower()\n",
    "\n",
    "sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "# 4. Set output:\n",
    "sb.dataio.dataset.set_output_keys(\n",
    "    datasets,\n",
    "    [\"id\", \"audio_id\", \"sig\", \"wrd\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0583294e-a9b2-4138-a67c-499fc353c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'audio_id': '20091124-0900-PLENARY-18-en_20091124-22:35:55_9',\n",
       " 'sig': tensor([[-0.9030, -1.0864, -2.8961,  ..., -1.2175, -1.3774, -1.0768],\n",
       "         [-0.3821, -0.0228, -0.3508,  ..., -1.2550, -0.8651, -0.9142],\n",
       "         [-0.9417, -0.8630, -1.0086,  ..., -0.9313, -0.4498, -0.4927],\n",
       "         ...,\n",
       "         [ 0.7211,  1.6580,  1.1608,  ..., -2.1000, -0.3901, -0.3237],\n",
       "         [-0.1314,  0.9283,  0.4485,  ..., -1.2711, -1.0480, -1.0994],\n",
       "         [-0.9046,  0.1171,  0.2605,  ..., -0.4674, -1.5554, -1.6531]]),\n",
       " 'wrd': 'and what about interoperability in the rail sector are national barriers preventing progress in this area as well or is there an unwillingness on the part of the rail industry to embrace the concept of interoperability'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets[0]\n",
    "print(len(dataset))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11c41fe-65c7-4c6a-9729-49a4ae8d002e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_batch_length': 100,\n",
       " 'num_buckets': 200,\n",
       " 'shuffle': False,\n",
       " 'batch_ordering': 'ascending',\n",
       " 'max_batch_ex': 256}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_hparams_train = hparams[\"dynamic_batch_sampler_train\"]\n",
    "dynamic_hparams_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f6e6d51-c804-4732-945a-009134045361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dynamic batch sampler\n",
    "train_batch_sampler = DynamicBatchSampler(\n",
    "    train_data,\n",
    "    length_func=lambda x: x[\"duration\"],\n",
    "    lengths_list=train_len_list,\n",
    "    **dynamic_hparams_train,\n",
    ")\n",
    "\n",
    "train_loader_kwargs = {\n",
    "    \"batch_sampler\": train_batch_sampler,\n",
    "    \"num_workers\": hparams[\"num_workers\"],\n",
    "}\n",
    "\n",
    "# create dataloader\n",
    "dataloader = sb.dataio.dataloader.make_dataloader(\n",
    "    dataset, **train_loader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c50ddb7-3562-4cc0-bf4f-e8d7452ff3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams[\"num_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7788e3c8-2c37-4787-ae34-c3a82537ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad089c-6782-4ad5-9031-83d61eb02b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e9d89a",
   "metadata": {},
   "source": [
    "# SENSE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c78f08fd-06fc-46e4-8838-e5b929870459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0107, -0.0148, -0.0010,  ...,  0.0458, -0.0136,  0.0232],\n",
       "        [ 0.0074, -0.0686, -0.0094,  ...,  0.0350, -0.0281, -0.0436],\n",
       "        [ 0.0521, -0.0670,  0.0395,  ...,  0.0693, -0.0990, -0.0329],\n",
       "        ...,\n",
       "        [ 0.0097,  0.0675,  0.0066,  ..., -0.0865, -0.0520, -0.0685],\n",
       "        [ 0.0313,  0.0098, -0.0862,  ..., -0.0320, -0.0016, -0.0751],\n",
       "        [-0.0229, -0.0170, -0.0042,  ..., -0.0045,  0.0349,  0.0505]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams[\"wav2vec2\"].model.encoder.layers[17].ffn1.intermediate_dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5965de11-86ac-45ea-bdf5-a438eccb11b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0107, -0.0148, -0.0010,  ...,  0.0458, -0.0136,  0.0232],\n",
       "        [ 0.0074, -0.0686, -0.0094,  ...,  0.0350, -0.0281, -0.0436],\n",
       "        [ 0.0521, -0.0670,  0.0395,  ...,  0.0693, -0.0990, -0.0329],\n",
       "        ...,\n",
       "        [ 0.0097,  0.0675,  0.0066,  ..., -0.0865, -0.0520, -0.0685],\n",
       "        [ 0.0313,  0.0098, -0.0862,  ..., -0.0320, -0.0016, -0.0751],\n",
       "        [-0.0229, -0.0170, -0.0042,  ..., -0.0045,  0.0349,  0.0505]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams[\"wav2vec2\"].model.encoder.layers[17].ffn1.intermediate_dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d11221a1-2e92-498e-89d7-6e4f33347eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0162,  0.0143, -0.0193,  ...,  0.0233, -0.0049, -0.0124]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams[\"model\"][0].attn_pooling_w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6309699-e11d-4049-82b7-64577b379330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0162,  0.0143, -0.0193,  ...,  0.0233, -0.0049, -0.0124]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams[\"model\"][0].attn_pooling_w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19033cf9-422f-435f-8e85-2127f3394850",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pretrainer\" in hparams.keys():\n",
    "    hparams[\"pretrainer\"].collect_files()\n",
    "    hparams[\"pretrainer\"].load_collected()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9d1c81c-4858-4033-9a84-1193bc19b172",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 14.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 33.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m w2v_bert \u001b[38;5;241m=\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m attention_pool \u001b[38;5;241m=\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mw2v_bert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m w2v_bert\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m attention_pool\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 14.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 33.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "w2v_bert = hparams[\"wav2vec2\"]\n",
    "attention_pool = hparams[\"model\"][0]\n",
    "\n",
    "w2v_bert.to(device)\n",
    "w2v_bert.eval()\n",
    "attention_pool.to(device)\n",
    "attention_pool.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e114d-ad5e-424e-8b9f-b02bd33d8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INTERVAL_MINUTES = 5\n",
    "CHECKPOINT_PATH = \"sense_checkpoint_test.pkl\"\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        print(f\"Resumed from batch {state['last_batch_index']}\")\n",
    "        return state[\"results\"], state[\"last_batch_index\"]\n",
    "    else:\n",
    "        return {}, -1\n",
    "\n",
    "def save_checkpoint(path, results, last_batch_index):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"results\": results,\n",
    "            \"last_batch_index\": last_batch_index\n",
    "        }, f)\n",
    "    print(f\"[Checkpoint] Saved batch {last_batch_index}, total items: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d5b3fa8-e54a-460e-be9f-79ab6c4369c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from batch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a0b60b3b34deb9ee9df23d04b7214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 14.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 30.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m w, wl \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39msig\n\u001b[0;32m---> 11\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwav2vec2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m feats \u001b[38;5;241m=\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m](feats)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/coros3/rwhetten/attention_alt/speechbrain/speechbrain/lobes/models/w2v_bert.py:151\u001b[0m, in \u001b[0;36mHuggingFaceWav2Vec2.forward\u001b[0;34m(self, wav)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(wav)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/coros3/rwhetten/attention_alt/speechbrain/speechbrain/lobes/models/w2v_bert.py:162\u001b[0m, in \u001b[0;36mHuggingFaceWav2Vec2.extract_features\u001b[0;34m(self, wav)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Takes an input waveform and return its corresponding wav2vec encoding.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mArguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    A batch of audio signals to transform to features.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#print(wav)\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_all_hiddens:\n\u001b[1;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py:1022\u001b[0m, in \u001b[0;36mWav2Vec2BertModel.forward\u001b[0;34m(self, input_features, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1018\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1020\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1022\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1024\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1027\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1028\u001b[0m     hidden_states,\n\u001b[1;32m   1029\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py:128\u001b[0m, in \u001b[0;36mWav2Vec2BertFeatureProjection.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# non-projected hidden states are needed for quantization\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 128\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states, norm_hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/aa/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 14.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 22.39 GiB is allocated by PyTorch, and 30.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "results, last_saved_batch = load_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    if i <= last_saved_batch:\n",
    "        continue  # Skip already processed batches\n",
    "\n",
    "    batch = batch.to(device)\n",
    "    w, wl = batch.sig\n",
    "    feats = hparams[\"wav2vec2\"](w)\n",
    "    feats = hparams[\"model\"][0](feats)\n",
    "    if i == 100:\n",
    "        print(feats.shape)\n",
    "        break\n",
    "    audio_ids = batch.audio_id\n",
    "    features = feats.cpu().numpy()\n",
    "    results.extend(zip(audio_ids, features))\n",
    "    # results.update(dict(zip(batch.audio_id, list(feats))))\n",
    "\n",
    "    # Checkpoint every X minutes\n",
    "    if (time.time() - start_time) >= SAVE_INTERVAL_MINUTES * 60:\n",
    "        save_checkpoint(CHECKPOINT_PATH, results, i)\n",
    "        start_time = time.time()  # Reset timer\n",
    "\n",
    "# Final save\n",
    "save_checkpoint(CHECKPOINT_PATH, results, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e63b8-8f9f-4271-ba81-6cf96140a24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8821e8c-b86a-4342-b106-e6e7fa3a71b0",
   "metadata": {},
   "source": [
    "# Test Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2e464ac-f4e1-46a2-80a8-79ec5c48003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from speechbrain.nnet.pooling import AttentionPooling\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "# Step 1: Define the model architecture\n",
    "# Assume the same hyperparameters were used during training\n",
    "attn_pooling1 = AttentionPooling(input_dim=1024)  # adjust sizes if known\n",
    "attn_pooling2 = AttentionPooling(input_dim=1024)\n",
    "\n",
    "# Step 2: Wrap in a ModuleList\n",
    "model = ModuleList([attn_pooling1, attn_pooling2])\n",
    "\n",
    "# Step 3: Load weights\n",
    "x = torch.load(\"/data/coros3/smdhaffar/SENSE/CKPT+checkpoint_epoch10/model.ckpt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f25c5ea-1492-4895-8737-1a3533bcd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.lobes.models.w2v_bert import HuggingFaceWav2Vec2\n",
    "\n",
    "w2v_bert = HuggingFaceWav2Vec2(\n",
    "    source=\"facebook/w2v-bert-2.0\",\n",
    "    output_norm=True,\n",
    "    save_path=\"/local_disk/apollon/rwhetten/sss_data_selection/sense/pt_store/wav2vec2_checkpoint\"\n",
    ")\n",
    "# wav2vec2: !new:speechbrain.lobes.models.w2v_bert.HuggingFaceWav2Vec2\n",
    "#     source: !ref <wav2vec_url>\n",
    "#     output_norm: True\n",
    "#     save_path: !ref <output_folder>/wav2vec2_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7be203-c433-4acc-9cc8-0132e042bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"/data/coros3/smdhaffar/SENSE/CKPT+checkpoint_epoch10/wav2vec2.ckpt\", map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7afc39-dd73-4a20-8d49-6753eb9fbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.utils.parameter_transfer import Pretrainer\n",
    "\n",
    "# Define references (normally these are modules like torch.nn.Module objects)\n",
    "# Replace with your actual module instances\n",
    "wav2vec2 = w2v_bert\n",
    "model = model\n",
    "\n",
    "# Paths (replace <sense_location> and <output_folder> with actual paths)\n",
    "sense_location = \"/data/coros3/smdhaffar/SENSE/CKPT+checkpoint_epoch10/\"\n",
    "output_folder = \"/local_disk/apollon/rwhetten/sss_data_selection/sense/pt_store\"\n",
    "\n",
    "# Create the Pretrainer instance\n",
    "pretrainer = Pretrainer(\n",
    "    collect_in=output_folder,\n",
    "    loadables={\n",
    "        \"wav2vec2\": wav2vec2,\n",
    "        \"model\": model,\n",
    "    },\n",
    "    paths={\n",
    "        \"wav2vec2\": f\"{sense_location}/wav2vec2.ckpt\",\n",
    "        \"model\": f\"{sense_location}/model.ckpt\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bd1ee82-ab7d-4177-bf05-1cec2e1c762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainer.collect_files()\n",
    "pretrainer.load_collected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3785d2-657a-4545-ad81-5075fa9b1245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceWav2Vec2(\n",
       "  (model): Wav2Vec2BertModel(\n",
       "    (feature_projection): Wav2Vec2BertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2BertEncoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2BertEncoderLayer(\n",
       "          (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn1): Wav2Vec2BertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn): Wav2Vec2BertSelfAttention(\n",
       "            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (distance_embedding): Embedding(73, 64)\n",
       "          )\n",
       "          (conv_module): Wav2Vec2BertConvolutionModule(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (glu): GLU(dim=1)\n",
       "            (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n",
       "            (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): SiLU()\n",
       "            (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn2): Wav2Vec2BertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f65b25-8287-49e8-ad51-6dce0017bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
