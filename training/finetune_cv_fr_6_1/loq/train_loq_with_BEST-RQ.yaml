# ################################
# Model: BEST-RQ + DNN + CTC
# Augmentation: SpecAugment
# Authors: Ryan Whetten 2025
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1234
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
output_folder: !ref results/bestrq_ctc_fr/<seed>
test_wer_file: !ref <output_folder>/wer_test.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# location of BEST-RQ pre-training checkpoing
hub: !PLACEHOLDER


train_csv: !ref <save_folder>/train.csv
tls_subset: !PLACEHOLDER
hf_hub: speechbrain/LoquaciousSet
hf_caching_dir: !PLACEHOLDER 
filter: false


####################### Training Parameters ####################################

number_of_epochs: 60
lr: 1.0
lr_bestrq: 0.0001
sorting: ascending
precision: fp16 # bf16, fp16 or fp32
sample_rate: 16000
ckpt_interval_minutes: 30 # save checkpoint every N min

num_workers: 2
max_batch_length_train: 300
max_batch_length_val: 100 # we reduce it as the beam is much wider (VRAM)
num_bucket: 200
shuffle: True # if true re-creates batches at each epoch shuffling examples.
batch_ordering: random
max_batch_ex: 256

dynamic_batch_sampler_train:
    max_batch_length: !ref <max_batch_length_train>
    num_buckets: !ref <num_bucket>
    shuffle: !ref <shuffle>
    batch_ordering: !ref <batch_ordering>
    max_batch_ex: !ref <max_batch_ex>

dynamic_batch_sampler_valid:
    max_batch_length: !ref <max_batch_length_val>
    num_buckets: !ref <num_bucket>
    shuffle: !ref <shuffle>
    batch_ordering: !ref <batch_ordering>
    max_batch_ex: !ref <max_batch_ex>

train_dataloader_opts:
    num_workers: !ref <num_workers>

test_dataloader_opts:
    batch_size: 2

# Streaming & dynamic chunk training options
# At least for the current architecture on LibriSpeech, we found out that
# non-streaming accuracy is very similar between `streaming: True` and
# `streaming: False`.
streaming: True  # controls all Dynamic Chunk Training & chunk size & left context mechanisms

# Configuration for Dynamic Chunk Training.
dynchunktrain_config_sampler: !new:speechbrain.utils.dynamic_chunk_training.DynChunkTrainConfigRandomSampler # yamllint disable-line rule:line-length
    chunkwise_prob: 0.6 # Probability during a batch to limit attention and sample a random chunk size in the following range
    chunk_size_min: 8 # Minimum chunk size (if in a DynChunkTrain batch)
    chunk_size_max: 32 # Maximum chunk size (if in a DynChunkTrain batch)
    limited_left_context_prob: 0.75 # If in a DynChunkTrain batch, the probability during a batch to restrict left context to a random number of chunks
    left_context_chunks_min: 2 # Minimum left context size (in # of chunks)
    left_context_chunks_max: 32 # Maximum left context size (in # of chunks)
    valid_config: !new:speechbrain.utils.dynamic_chunk_training.DynChunkTrainConfig
        chunk_size: 8
        left_context_size: 16

# BPE parameters
token_type: char  # ["unigram", "bpe", "char"]
character_coverage: 1.0

####################### Model Parameters #######################################
# activation: !name:torch.nn.LeakyReLU
pt_model_output_dim: 592
dnn_neurons: 1024
dropout: 0.15


# Transformer
d_model: !ref <pt_model_output_dim>
nhead: 8
num_encoder_layers: 12
num_decoder_layers: 0
d_ffn: 2048
transformer_dropout: 0.1
activation: !name:torch.nn.GELU
output_neurons: 5000
encoder_layerdrop: 0.05
attention_type: RoPEMHA
encoder_module: conformer
dnn_activation: !new:torch.nn.LeakyReLU
freeze_bestrq: False

# Feature parameters
n_fft: 400
n_mels: 80

# Outputs
output_neurons_ctc: 29  # BPE size, index(blank/eos/bos) = 0

# Decoding parameters
# Be sure that the bos and eos index match with the BPEs ones
blank_index: 0
bos_index: 1
eos_index: 2

# Decoding parameters
test_beam_search:
    blank_index: !ref <blank_index>
    beam_size: 100
    beam_prune_logp: -12.0
    token_prune_min_logp: -1.2
    prune_history: True
    topk: 1
    alpha: 1.0
    beta: 0.5
    # To use n-gram LM for decoding, follow steps in README.md.
    # kenlm_model_path: none
#
# Functions and classes
#
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

############################## Augmentations ###################################

# Speed perturbation
speed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb
    orig_freq: !ref <sample_rate>
    speeds: [95, 100, 105]

# Frequency drop: randomly drops a number of frequency bands to zero.
drop_freq: !new:speechbrain.augment.time_domain.DropFreq
    drop_freq_low: 0
    drop_freq_high: 1
    drop_freq_count_low: 1
    drop_freq_count_high: 3
    drop_freq_width: 0.05

# Time drop: randomly drops a number of temporal chunks.
drop_chunk: !new:speechbrain.augment.time_domain.DropChunk
    drop_length_low: 1000
    drop_length_high: 2000
    drop_count_low: 1
    drop_count_high: 5

# Augmenter: Combines previously defined augmentations to perform data augmentation
wav_augment: !new:speechbrain.augment.augmenter.Augmenter
    min_augmentations: 3
    max_augmentations: 3
    augment_prob: 1.0
    augmentations: [
        !ref <speed_perturb>,
        !ref <drop_freq>,
        !ref <drop_chunk>]

############################## Models ##########################################

# DNN
back_end_ffn: !new:speechbrain.nnet.containers.Sequential
    input_shape: [null, null, !ref <pt_model_output_dim>]
    linear1: !name:speechbrain.nnet.linear.Linear
        n_neurons: !ref <dnn_neurons>
        bias: True
    bn1: !name:speechbrain.nnet.normalization.BatchNorm1d
    activation: !new:torch.nn.LeakyReLU
    drop: !new:torch.nn.Dropout
        p: !ref <dropout>
    linear2: !name:speechbrain.nnet.linear.Linear
        n_neurons: !ref <dnn_neurons>
        bias: True
    bn2: !name:speechbrain.nnet.normalization.BatchNorm1d
    activation2: !new:torch.nn.LeakyReLU
    drop2: !new:torch.nn.Dropout
        p: !ref <dropout>
    linear3: !name:speechbrain.nnet.linear.Linear
        n_neurons: !ref <dnn_neurons>
        bias: True
    bn3: !name:speechbrain.nnet.normalization.BatchNorm1d
    activation3: !new:torch.nn.LeakyReLU


# BEST-RQ (mel-spec, normalize, CNN, transformer)
compute_features: !new:speechbrain.lobes.features.Fbank
   sample_rate: !ref <sample_rate>
   n_fft: !ref <n_fft>
   n_mels: !ref <n_mels>
  
normalize: !new:speechbrain.processing.features.InputNormalization
   norm_type: global
   update_until_epoch: 0

CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd
   input_shape: (8, 10, 80)
   num_blocks: 2
   num_layers_per_block: 1
   out_channels: (128, 32)
   kernel_sizes: (5, 5)
   strides: (2, 2)
   residuals: (False, False)

Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR # yamllint disable-line rule:line-length
   input_size: 640
   tgt_vocab: !ref <output_neurons>
   d_model: !ref <d_model>
   nhead: !ref <nhead>
   num_encoder_layers: !ref <num_encoder_layers>
   num_decoder_layers: !ref <num_decoder_layers>
   d_ffn: !ref <d_ffn>
   dropout: !ref <transformer_dropout>
   activation: !ref <activation>
   conformer_activation: !ref <activation>
   encoder_module: !ref <encoder_module>
   attention_type: !ref <attention_type>
   normalize_before: True
   causal: False
   layerdrop_prob: !ref <encoder_layerdrop>

enc: !new:speechbrain.lobes.models.transformer.TransformerASR.EncoderWrapper
   transformer: !ref <Transformer>

pt_model: !new:torch.nn.ModuleList
   - [!ref <CNN>, !ref <enc>]


# wav2vec2: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2
#     source: !ref <wav2vec2_hub>
#     output_norm: False
#     freeze: !ref <freeze_wav2vec>
#     freeze_feature_extractor: !ref <freeze_feature_extractor>
#     save_path: !ref <wav2vec2_folder>

#####
# Uncomment this block if you prefer to use a Fairseq pretrained model instead
# of a HuggingFace one. Here, we provide an URL that is obtained from the
# Fairseq github for the multilingual XLSR.
#
#wav2vec2_url: https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt
#wav2vec2: !new:speechbrain.lobes.models.fairseq_wav2vec.FairseqWav2Vec2
#    pretrained_path: !ref <wav2vec2_url>
#    output_norm: True
#    freeze: False
#    save_path: !ref <save_folder>/wav2vec2_checkpoint/model.pt
#####


ctc_lin: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <dnn_neurons>
    n_neurons: !ref <output_neurons_ctc>

log_softmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

ctc_cost: !name:speechbrain.nnet.losses.ctc_loss
    blank_index: !ref <blank_index>

modules:
    normalize: !ref <normalize>
    CNN: !ref <CNN>
    enc: !ref <enc>
    pt_model: !ref <pt_model>
    back_end_ffn: !ref <back_end_ffn>
    ctc_lin: !ref <ctc_lin>

model: !new:torch.nn.ModuleList
    - [!ref <back_end_ffn>, !ref <ctc_lin>]

model_opt_class: !name:torch.optim.Adadelta
    lr: !ref <lr>
    rho: 0.95
    eps: 1.e-8

bestrq_opt_class: !name:torch.optim.Adam # maybe test AdamW
    lr: !ref <lr_bestrq>

lr_annealing_model: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 0.0025
    annealing_factor: 0.8
    patient: 0

lr_annealing_bestrq: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr_bestrq>
    improvement_threshold: 0.0025
    annealing_factor: 0.9
    patient: 0

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        pt_model: !ref <pt_model>
        normalize: !ref <normalize>
        scheduler_model: !ref <lr_annealing_model>
        scheduler_bestrq: !ref <lr_annealing_bestrq>
        counter: !ref <epoch_counter>

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    collect_in: !ref <save_folder>
    loadables:
        pt_model: !ref <pt_model>
        normalize: !ref <normalize>
    paths:
        pt_model: !ref <hub>/model.ckpt
        normalize: !ref <hub>/normalizer.ckpt

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats

cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
    split_tokens: True
